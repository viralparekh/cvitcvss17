{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to fine-tune the pre-trained VGG-Face descriptors for the task of classifying the gender of a person from his/her face image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2, math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.legacy import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code to load and pre-process image for VGG-Face has been encapsulated within a function\n",
    "\n",
    "def loadImage(imgPath):\n",
    "    inputImg = cv2.imread(imgPath)\n",
    "\n",
    "    # re-scale the smaller dim (among width, height) to refSize\n",
    "    refSize, targetSize = 256, 224\n",
    "    imgRows, imgCols = inputImg.shape[0], inputImg.shape[1]\n",
    "    if imgCols < imgRows: resizedImg = cv2.resize(inputImg, (refSize, refSize * imgRows / imgCols))\n",
    "    else: resizedImg = cv2.resize(inputImg, (refSize * imgCols / imgRows, refSize))\n",
    "\n",
    "    # center-crop\n",
    "    oH, oW = targetSize, targetSize\n",
    "    iH, iW = resizedImg.shape[0], resizedImg.shape[1]\n",
    "    anchorH, anchorW = int(math.ceil((iH - oH)/2)), int(math.ceil((iW - oW) / 2))\n",
    "    croppedImg = resizedImg[anchorH:anchorH+oH, anchorW:anchorW+oW]\n",
    "\n",
    "    # convert shape from (height, width, 3) to (3, width, height)\n",
    "    channel_1, channel_2, channel_3 = croppedImg[:, :, 0], croppedImg[:, :, 1], croppedImg[:, :, 2]\n",
    "    croppedImg = np.empty([3, croppedImg.shape[0], croppedImg.shape[1]])\n",
    "    croppedImg[0], croppedImg[1], croppedImg[2] = channel_1, channel_2, channel_3\n",
    "\n",
    "    # subtract training mean\n",
    "    inputImg = inputImg.astype(float)\n",
    "    trainingMean = [129.1863, 104.7624, 93.5940]\n",
    "    for i in range(3): croppedImg[i] = croppedImg[i] - trainingMean[i]\n",
    "    return croppedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to compute pre-trained VGG-Face descriptors for a set of images \n",
    "\n",
    "def getVggFeatures(imgPaths, preTrainedNet):\n",
    "    nImgs = len(imgPaths)\n",
    "    vggFace.modules[31] = nn.View(nImgs, 25088)\n",
    "    \n",
    "    batchInput = torch.Tensor(nImgs, 3, 224, 224)\n",
    "    for i in range(nImgs): batchInput[i] = torch.from_numpy(loadImage(imgPaths[i]))\n",
    "    \n",
    "    batchOutput = preTrainedNet.forward(batchInput)\n",
    "    return preTrainedNet.modules[35].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network structure that we'll train for gender classification\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(4096, 2)\n",
    "        self.softmax = torch.nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy (before training) =  0.65\n",
      "epoch  1 / 1 : loss =  0.635333937127\n",
      "accuracy (after training) =  0.95\n"
     ]
    }
   ],
   "source": [
    "# fix the seeds of random number generators\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# load the dataset and the pre-trained network\n",
    "vggFace = load_lua(\"preTrainedNets/VGG_FACE_pyTorch_small.t7\")\n",
    "dataset = load_lua(\"datasets/Experiment_2/celeba-gender-dataset.t7\")\n",
    "\n",
    "# initialize the net, loss and optimizer (SGD)\n",
    "net = Net()\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00005, momentum=0.9, weight_decay=0.0005)\n",
    "nEpochs, batchSize = 1, 10\n",
    "\n",
    "# define a function to test the performance of our model on a separate test set\n",
    "def evaluate(net, dataset):\n",
    "    correctPreds = 0.0\n",
    "    for startIdx in range(0, dataset['testset'].size, batchSize):\n",
    "        endIdx = min(startIdx + batchSize - 1, dataset['testset'].size - 1)\n",
    "        size = (endIdx - startIdx + 1)\n",
    "        \n",
    "        batchInput, batchLabel = torch.Tensor(size, 4096), torch.LongTensor(size)\n",
    "        batchImgPaths = []\n",
    "        for offset in range(size):\n",
    "            imgPath = \"datasets/Experiment_2/\" + dataset['testset'].imgPaths[startIdx+offset]\n",
    "            batchImgPaths.append(imgPath)\n",
    "            label = dataset['testset'].labels[startIdx+offset]\n",
    "            batchLabel[offset] = int(label)\n",
    "        \n",
    "        batchInput = getVggFeatures(batchImgPaths, vggFace)\n",
    "        batchInput = Variable(batchInput)\n",
    "\n",
    "        batchOutput = net(batchInput)\n",
    "        batchOutput, batchLabel = batchOutput.data.numpy(), batchLabel.numpy()\n",
    "        predictions = np.argmax(batchOutput, 1)\n",
    "        correctPreds += np.sum(predictions == batchLabel)\n",
    "    return correctPreds / dataset['testset'].size\n",
    "\n",
    "print \"accuracy (before training) = \", evaluate(net, dataset)\n",
    "\n",
    "# start training\n",
    "\n",
    "for epochCtr in range(nEpochs):\n",
    "    \n",
    "    shuffle = np.random.permutation(dataset['trainset'].size)\n",
    "    runningLoss, iterCnt = 0.0, 0\n",
    "    for startIdx in range(0, dataset['trainset'].size, batchSize):\n",
    "        endIdx = min(startIdx + batchSize - 1, dataset['trainset'].size-1)\n",
    "        size = (endIdx - startIdx + 1)\n",
    "    \n",
    "        batchInput, batchLabel = torch.Tensor(size, 4096), torch.LongTensor(size)\n",
    "        batchImgPaths = []\n",
    "        for offset in range(size):\n",
    "            imgPath = \"datasets/Experiment_2/\" + dataset['trainset'].imgPaths[shuffle[startIdx+offset]]\n",
    "            batchImgPaths.append(imgPath)\n",
    "            label = dataset['trainset'].labels[shuffle[startIdx+offset]]\n",
    "            batchLabel[offset] = int(label) \n",
    "        \n",
    "        batchInput = getVggFeatures(batchImgPaths, vggFace)\n",
    "        batchInput = Variable(batchInput)\n",
    "        batchLabel = Variable(batchLabel)\n",
    "        batchOutput = net(batchInput)\n",
    "        loss = criterion(batchOutput, batchLabel)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        runningLoss += loss.data[0]\n",
    "        iterCnt += 1\n",
    "\n",
    "    print \"epoch \", (epochCtr+1), \"/\", nEpochs, \": loss = \", runningLoss/iterCnt\n",
    "\n",
    "print \"accuracy (after training) = \", evaluate(net, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
