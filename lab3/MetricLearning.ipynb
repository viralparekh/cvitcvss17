{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Learning\n",
    "In the previous tutorial, we saw how the pre-trained VGG-Face representations, combined with a distance metric (L2-distance) helped us in the problem of face verification i.e. determining whether two face images belong to the same identity or not. \n",
    "\n",
    "But we know that metrics such as L1 or L2 may not always be optimal for the task at hand. So in this tutorial, we will see if we can \"learn\" such distance metrics from the data, and whether that leads to an improvement in verification peformance.\n",
    "\n",
    "There are cases where a simple L2 distance in the representation space may not preserve the semantic similarity between instances of the same class. Consider the example shown below. Let us assume that the squares and the circles represent instances belonging to two different classes. It is evident that the original D-dimensional representation space does not do a good job in separating the two classes. In such cases, we attempt to learn a $d \\times D$ matrix L from the data such that the points are separated in a much better fashion in the projected space. That is, the L2-distances between pairs of points $x_i$ and $x_j$ in the projected space, given by $ ||\\;Lx_i - Lx_j\\;||_2^2 $ are such that similar points are brought closer together whereas dissimilar points are pushed apart.\n",
    "\n",
    "<img src=\"images/ml-proj.png\">\n",
    "\n",
    "Such forms of metric learning approaches serve a dual purpose -- (a) they help in learning better metrics by bringing similar points closer and pushng dissimilar points away, and (b) they help in learning a compact and discriminative d-dimensional representation where $ d << D $ i.e. they also help in reducing the dimensionality of our face descriptors thereby making them suitable for large-scale applications.\n",
    "\n",
    "In this tutorial, we will see how we can use face pairs from the CFPW dataset to learn a projection matrix that helps us generate compact and discriminative low-dimensional projections. We will also see whether these compact descriptors lead to a better verification performance than what we observed in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2, math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.serialization import load_lua\n",
    "from torch.legacy import nn\n",
    "\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define certain utility functions to load, pre-process data and also generate the pre-trained face descriptors. This should be familiar to you by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadImage(imgPath):\n",
    "    inputImg = cv2.imread(imgPath)\n",
    "\n",
    "    # re-scale the smaller dim (among width, height) to refSize\n",
    "    refSize, targetSize = 256, 224\n",
    "    imgRows, imgCols = inputImg.shape[0], inputImg.shape[1]\n",
    "    if imgCols < imgRows: resizedImg = cv2.resize(inputImg, (refSize, refSize * imgRows / imgCols))\n",
    "    else: resizedImg = cv2.resize(inputImg, (refSize * imgCols / imgRows, refSize))\n",
    "\n",
    "    # center-crop\n",
    "    oH, oW = targetSize, targetSize\n",
    "    iH, iW = resizedImg.shape[0], resizedImg.shape[1]\n",
    "    anchorH, anchorW = int(math.ceil((iH - oH)/2)), int(math.ceil((iW - oW) / 2))\n",
    "    croppedImg = resizedImg[anchorH:anchorH+oH, anchorW:anchorW+oW]\n",
    "\n",
    "    # convert shape from (height, width, 3) to (3, width, height)\n",
    "    channel_1, channel_2, channel_3 = croppedImg[:, :, 0], croppedImg[:, :, 1], croppedImg[:, :, 2]\n",
    "    croppedImg = np.empty([3, croppedImg.shape[0], croppedImg.shape[1]])\n",
    "    croppedImg[0], croppedImg[1], croppedImg[2] = channel_1, channel_2, channel_3\n",
    "\n",
    "    # subtract training mean\n",
    "    inputImg = inputImg.astype(float)\n",
    "    trainingMean = [129.1863, 104.7624, 93.5940]\n",
    "    for i in range(3): croppedImg[i] = croppedImg[i] - trainingMean[i]\n",
    "    return croppedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVggFeatures(imgPaths, preTrainedNet):\n",
    "    nImgs = len(imgPaths)\n",
    "    preTrainedNet.modules[31] = nn.View(nImgs, 25088)\n",
    "    preTrainedNet = preTrainedNet.cuda()\n",
    "    \n",
    "    batchInput = torch.Tensor(nImgs, 3, 224, 224)\n",
    "    for i in range(nImgs): batchInput[i] = torch.from_numpy(loadImage(imgPaths[i]))\n",
    "    \n",
    "    batchOutput = preTrainedNet.forward(batchInput.cuda())\n",
    "    return preTrainedNet.modules[35].output.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "Recall that our aim is to learn a simple projection matrix over the pre-trained, D = 4096-d face representations so that the resultant compact descriptors are able to better separate face pairs in terms of identity (become better at face verification). we also know that the operation performed by an fc-layer is exactly the same as a matrix-vector product. So, our network architecture would essentially consist of learning a d = 64 dimensional linear layer over the pre-trained VGG-Face descriptors.\n",
    "\n",
    "However, there is a difference in the manner in which we are going to train this network. Traditional CNN architectures for classification are trained using images and their corresponding class labels. Here, however, our network is learning to separate(bring together) (dis)similar face pairs. This means that the training should also happen in the form of image pairs where the label asosciated with each image pair during training tells us whether the faces in the pair belong to the same identity or not.\n",
    "\n",
    "A conceptually convenient way to represent such networks schematically is to visualize this as two identical networks (which share all weights and biases) where each of the network accepts one image from the pair and computes the respective representations. Such networks are called Siamese networks (figure below).\n",
    "\n",
    "<img src=\"images/siamese.png\">\n",
    "\n",
    "Keeping this in mind, we define the architcture of our Siamese netowrk in PyTorch. Note that the final output of our network is not some feature vector or a vector of log-likelihoods. Instead, the output is the L2 distance between the representations that have been computed by each branch of the Siamese network. During the process of training, the network learns to minimize/maximize this L2 distance depending upon whether the input images are similar/dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Siamese(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(4096, 64)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        o1 = self.fc1(x1)\n",
    "        o2 = self.fc1(x2)\n",
    "        o = torch.sqrt(torch.sum(torch.mul(o1-o2, o1-o2), 1))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subtle point that needs to be remembered is that the two networks in the Siamese architecture are merely for representational convenience. There is only a single network which exists in memory at all times.\n",
    "\n",
    "We have consolidated the code for getting L2 distances between pairs of images in our dataset in the form of a function definition. This code is similar to the one that we saw in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(net, dataset):\n",
    "    nPairs, batchSize = len(dataset['pairs']), 10\n",
    "    classifierScores, labels = [], []\n",
    "    \n",
    "    for startIdx in range(0, nPairs, batchSize):\n",
    "        endIdx = min(startIdx+batchSize-1, nPairs-1)\n",
    "        size = (endIdx - startIdx + 1)\n",
    "\n",
    "        imgPaths1, imgPaths2, batchLabels = [], [], []\n",
    "        for offset in range(size):\n",
    "            pair = dataset['pairs'][startIdx+offset]\n",
    "            imgPaths1.append(\"../../data/lab3/Experiment_3/\" + pair.img1)\n",
    "            imgPaths2.append(\"../../data/lab3/Experiment_3/\" + pair.img2)\n",
    "            batchLabels.append(int(pair.label) * -1)\n",
    "    \n",
    "        descrs1 = getVggFeatures(imgPaths1, vggFace).clone()\n",
    "        descrs2 = getVggFeatures(imgPaths2, vggFace).clone()\n",
    "        batchOutput = net(Variable(descrs1).cuda(), Variable(descrs2).cuda())\n",
    "        \n",
    "        classifierScores += batchOutput.data.cpu().numpy().T[0].tolist()\n",
    "        labels += batchLabels\n",
    "    \n",
    "    return classifierScores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, classifierScores, labels):\n",
    "        self.scores = classifierScores\n",
    "        self.labels = labels\n",
    "\n",
    "    def getAvgDist(self):\n",
    "        nSim, nDiss = 0, 0\n",
    "        avgDistSim, avgDistDiss = 0.0, 0.0\n",
    "        for i in range(len(self.scores)):\n",
    "            if self.labels[i] == 1: \n",
    "                avgDistDiss += self.scores[i] \n",
    "                nDiss += 1\n",
    "            else: \n",
    "                avgDistSim += self.scores[i]\n",
    "                nSim += 1\n",
    "        return avgDistSim/nSim, avgDistDiss/nDiss\n",
    "    \n",
    "    def getROC(self):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(self.labels, self.scores)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        eer, r = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1., full_output=True)\n",
    "        return eer, auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset (CFPW) remains the same as the last tutorial. As always, we also load the pre-trained VGG-Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vggFace = load_lua(\"../../data/lab3/VGG_FACE_pyTorch_small.t7\")\n",
    "dataset = load_lua(\"../../data/lab3/Experiment_3/cfpw-facePairs-dataset.t7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the Siamese network architecture and some other hyperparameters related to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "net = Siamese()\n",
    "criterion = torch.nn.HingeEmbeddingLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.00005, weight_decay=0.0005)\n",
    "\n",
    "net = net.cuda()\n",
    "criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "The loss function that we use here is called the Hinge Embedding loss. It's formulation is given below -- \n",
    "\n",
    "loss $ (x, y) = \\left( \\frac{1+y}{2} \\right) \\; x + \\left ( \\frac{1-y}{2} \\right ) \\max\\;(0, margin \\; - \\; x) $\n",
    "\n",
    "where x is the L2 distance between the pair of input images and $y \\in \\{ +1, -1 \\}$ is the label for the image pair. As you can see, for similar image pairs with a class label of y = 1, the loss tries to minimize the L2 distance whereas for dissimilar image pairs, it tries to push the distance to be greater than the hyperparameter margin.\n",
    "\n",
    "Before we start training, let us look at the verification metrics on our dataset of 100 image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgDistSim =  11.5818845654 , avgDistDiss =  12.8276516151\n",
      "EER =  0.39 , AUC =  0.6484\n"
     ]
    }
   ],
   "source": [
    "# Before training\n",
    "\n",
    "scores, labels = evaluate(net, dataset)\n",
    "verifMetric = Metrics(scores, labels)\n",
    "avgDistSim, avgDistDiss = verifMetric.getAvgDist()\n",
    "print \"avgDistSim = \", avgDistSim, \", avgDistDiss = \", avgDistDiss\n",
    "\n",
    "eer, auc, fpr, tpr = verifMetric.getROC()\n",
    "print \"EER = \", eer, \", AUC = \", auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given below is the code for training our Siamese network using image pairs from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 / 2 : loss =  5.76211078167\n",
      "epoch  1 / 2 : loss =  4.93768441677\n"
     ]
    }
   ],
   "source": [
    "nEpochs, nPairs, batchSize = 2, len(dataset['pairs']), 10\n",
    "for epochCtr in range(nEpochs):\n",
    "    \n",
    "    shuffle = np.random.permutation(nPairs)\n",
    "    runningLoss, iterCnt = 0.0, 0\n",
    "    for startIdx in range(0, nPairs, batchSize):\n",
    "        endIdx = min(startIdx + batchSize - 1, nPairs - 1)\n",
    "        size = endIdx - startIdx + 1\n",
    "    \n",
    "        imgPaths1, imgPaths2, labels = [], [], []\n",
    "        for offset in range(size):\n",
    "            pair = dataset['pairs'][shuffle[startIdx+offset]]\n",
    "            imgPaths1.append(\"../../data/lab3/Experiment_3/\" + pair.img1)\n",
    "            imgPaths2.append(\"../../data/lab3/Experiment_3/\" + pair.img2)\n",
    "            labels.append(int(pair.label))\n",
    "        \n",
    "        descrs1 = getVggFeatures(imgPaths1, vggFace).clone()\n",
    "        descrs2 = getVggFeatures(imgPaths2, vggFace).clone()\n",
    "        \n",
    "        batchOutput = net(Variable(descrs1).cuda(), Variable(descrs2).cuda())\n",
    "        loss = criterion(batchOutput, Variable(torch.Tensor(labels)).cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        runningLoss += loss.data[0]\n",
    "        iterCnt += 1\n",
    "    \n",
    "    print \"epoch \", epochCtr, \"/\", nEpochs, \": loss = \", runningLoss/iterCnt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we are done training, let us take a look at the same metrics once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avgDistSim =  8.24356043816 , avgDistDiss =  12.2217759705\n",
      "EER =  0.153333333333 , AUC =  0.9352\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "\n",
    "scores, labels = evaluate(net, dataset)\n",
    "verifMetric = Metrics(scores, labels)\n",
    "avgDistSim, avgDistDiss = verifMetric.getAvgDist()\n",
    "print \"avgDistSim = \", avgDistSim, \", avgDistDiss = \", avgDistDiss\n",
    "\n",
    "eer, auc, fpr, tpr = verifMetric.getROC()\n",
    "print \"EER = \", eer, \", AUC = \", auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that there is a significant increase in the verification performance as a result of metric learning. Also, note that the gap between the distances of the similar and dissimilar face pairs has increased. What makes these results even more interesting is the fact that these values are better than what we obtained in the previous tutorial. Recall that previously, we were working with 4096-d descriptors whereas now we have only 64-d descriptors.\n",
    "\n",
    "### Exercises\n",
    "1. These AUC metric that we obtain for verification prior to training is significantly worse than what we observed in the previous tutorial. Why is that so?\n",
    "\n",
    "2. As part of the binary classification metrics, we have computed a quantity called the EER which stands for Equal Error Rate. What does it represent? By looking at the way it has been evaluated and the documentation for the functions used, can you figure out it's relation to fpr/tpr?\n",
    "\n",
    "3. Also, given the code to compute EER, go back to the previous tutorial on Face Verification and compute the EER for the CFPW and the LFW datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
