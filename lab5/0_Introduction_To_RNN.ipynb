{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SEQUENCE MODELING\n",
    "\n",
    "Many forms of data are sequential in nature. For example, consider textual data.\n",
    "\n",
    "\n",
    "\n",
    "\"Where there is righteousness in the heart\n",
    "There is beauty in the character.\n",
    "When there is beauty in the character,\n",
    "There is harmony in the home.\n",
    "When there is harmony in the home.\n",
    "There is an order in the nation.\n",
    "When there is order in the nation,\n",
    "There is peace in the world.\"\n",
    "\n",
    "       -- Dr. A P J Abdul Kalam\n",
    "                                                                      \n",
    "\n",
    "Note, how the sentences are related to each other. There are dependency structures within the paragraph which makes certain words more likely. For example, consider the statement \"When there is order in the ...\". From the context, we can predict that the next word is likely to be nation. We want neural networks to capture such dependencies in our data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RECURRENCE RELATION\n",
    "\n",
    "A natural way to modelsequential dependencies is through a recurrence relation\n",
    "\n",
    "\n",
    "$ h_t = f_W(h_{t-1}, x_t) $\n",
    "\n",
    "where,\n",
    "\n",
    "$h_t$ denotes state at time step t\n",
    "\n",
    "$h_{t-1}$ denotes state at time step $t-1$\n",
    "\n",
    "$f_W$ denotes a function with parameters $W$\n",
    "\n",
    "$x_t$ denotes input at time step $t$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RECURRENT NETWORK\n",
    "\n",
    "The recurrence relation can be realised by a neural network with a feedback connection:\n",
    "<img src=\"images/rnn.png\" alt=\"rnn\" style=\"height: 200px;\"/>\n",
    "\n",
    "We can generalize this to a network with many hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### UNROLLED RECURRENT NEURAL NETWORK\n",
    ")\n",
    "A RNN unrolled to few number of timesteps is a directed acyclic graph (DAG). So, we can still use backpropagation to update parameters of the network. Such, a backpropagation is called backpropagation through time (BPTT)\n",
    "\n",
    "<img src=\"images/unrolled_rnn.png\" alt=\"rnn\" style=\"height: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN recurrence in matrix form\n",
    "\n",
    "$ h_{t}^{l} = \\tanh W^{l} \\left[h_{t}^{l-1} \\; h_{t-1}^{l}\\right]^{T}  $\n",
    "\n",
    "where\n",
    "\n",
    "$ h \\in R^{n} $\n",
    "\n",
    "$W^{l}$ is $n \\times 2n $ matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PROBLEM WITH RNN: VANISHING GRADIENTS\n",
    "\n",
    "\n",
    "Visualization by Pranav V Shyam: Error generated at 128th time step is propagated back. Numbers at the top indicate epoch. LSTM stands for long short term memory. [ Precise definition later. Lets understand a high level picture first ]  \n",
    "\n",
    "<img src=\"images/RNN vs LSTM Vanishing Gradients - Imgur.gif\" alt=\"rnn\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RECALL: VANISHING GRADIENT PROBLEM IN CNN\n",
    "    \n",
    "<img src=\"images/vanishing_gradients_cnn.png\" alt=\"rnn\" style=\"height: 400px;\"/>\n",
    "    \n",
    "    \n",
    "A solution to vanishing gradients and thereby increasing depth was proposed by Kaiming He et al called [ResNet](https://arxiv.org/pdf/1512.03385.pdf) The figure from He et al, illustrates the problem. On the left, a CNN with 34 layers performs poorly as compared to a shallower 18 layer one. To address this problem and obtain results similar to one on the right, they indroduce a concept of \"residual blocks\"\n",
    "\n",
    "#### CAUSE\n",
    "\n",
    "Error and gradient values backpropagated through a very deep network keep on diminishing by repeated multimplication with numbers in the range [0, 0.1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RECALL: RESIDUAL BLOCK\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/residual_block.png\" alt=\"rnn\" style=\"height: 300px;\"/>\n",
    "\n",
    "\n",
    "A residual block introduces \"skip connections\" for (residual) learning of functions of the form:\n",
    "\n",
    "$$ F(x) + x $$\n",
    "\n",
    "Due to the identity mapping the gradient values are biased to be closer to 1. The skip connections ensure pathways for the gradients to backpropagate without dying down. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VANISHING GRADIENTS IN RNN\n",
    "\n",
    "\n",
    "In RNN vanishing vanishing gradient problem can happen over time dimension\n",
    "\n",
    "<img src=\"images/rnn2.png\" alt=\"rnn\" style=\"height: 300px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$ h_{t}^{l} = \\tanh W^{l} \\left[h_{t}^{l-1} \\; h_{t-1}^{l}\\right]^{T}  $\n",
    "\n",
    "where\n",
    "\n",
    "$ h \\in R^{n} $\n",
    "\n",
    "$W^{l}$ is $n \\times 2n $ matrix \n",
    "\n",
    "The matrix $W^{l}$ gets multiplied again over timesteps. If the eigenvalues of the matrix are less than 1, then it can lead to vanishing gradient problem. \n",
    "\n",
    "[Note: two blocks in hidden layers: In addition to hidden state, we use another variable called cell state. More later]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM INTUITION: LSTM  \n",
    "\n",
    "<img src=\"images/rnn_lstm.png\" alt=\"rnn\" style=\"height: 400px;\"/>\n",
    "\n",
    "LSTM introduces an additive interaction in the cell states. Similar, to skip connections it offers pathways for non-zero gradient backprogations. Thus prevents, vanishing gradient problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM CELLS\n",
    "\n",
    "<img src=\"images/lstm.png\" alt=\"rnn\" style=\"height: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM EQUATIONS\n",
    "\n",
    "$$\n",
    "Input:  i = sigmoid(W^{l} \\left[h_{t}^{l-1} \\; h_{t-1}^{l}\\right]^{T}) \\\\\n",
    "Forget: f = sigmoid(W^{l} \\left[h_{t}^{l-1} \\; h_{t-1}^{l}\\right]^{T}) \\\\\n",
    "Output: o = sigmoid(W^{l} \\left[h_{t}^{l-1} \\; h_{t-1}^{l}\\right]^{T}) \\\\\n",
    "        g = \\tanh(W^{l} \\left[h_{t}^{l-1} \\; h_{t-1}^{l}\\right]^{T}) \\\\\n",
    "        c_{t}^{l} = f \\odot c_{t-1}^{l} + i \\odot g \\\\\n",
    "        h_{t}^{l} = o \\odot \\tanh(c_{t}^{l})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Following references have been used to make this tutorial:\n",
    "\n",
    "1. Andrej Karpathy, Justin Johnson [\"CS231n: Convolutional Neural Networks for Visual Recognition\"](http://cs231n.stanford.edu/) Stanford University\n",
    "2. Andrej Karpathy, [Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "3. Christopher Olah [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "4. DeepLearning4j [Beginner's guide to learning LSTMs](https://deeplearning4j.org/lstm.html)\n",
    "5. He, Kaiming, et al. [\"Deep residual learning for image recognition.\"](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
