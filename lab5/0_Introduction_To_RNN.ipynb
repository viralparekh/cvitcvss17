{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SEQUENCE MODELING\n",
    "\n",
    "Many forms of data are sequential in nature. For example, consider textual data.\n",
    "\n",
    "\n",
    "\n",
    "\"Where there is righteousness in the heart\n",
    "There is beauty in the character.\n",
    "When there is beauty in the character,\n",
    "There is harmony in the home.\n",
    "When there is harmony in the home.\n",
    "There is an order in the nation.\n",
    "When there is order in the nation,\n",
    "There is peace in the world.\"\n",
    "\n",
    "       -- Dr. A P J Abdul Kalam\n",
    "                                                                      \n",
    "\n",
    "Note, how the sentences are related to each other. There are dependency structures within the paragraph which makes certain words more likely. For example, consider the statement \"When there is order in the ...\". From the context, we can predict that the next word is likely to be nation. We want neural networks to capture such dependencies in our data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RECURRENCE RELATION\n",
    "\n",
    "A natural way to handle sequential dependencies is through a recurrence relation\n",
    "\n",
    "\n",
    "$ h_t = f_W(h_{t-1}, x_t) $\n",
    "\n",
    "where,\n",
    "\n",
    "$h_t$ denotes state at time step t\n",
    "\n",
    "$h_{t-1}$ denotes state at time step $t-1$\n",
    "\n",
    "$f_W$ denotes a function with parameters $W$\n",
    "\n",
    "$x_t$ denotes input at time step $t$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RECURRENT NETWORK\n",
    "\n",
    "The recurrence relation can be realised by a neural network with a feedback connection:\n",
    "<img src=\"images/rnn.png\" alt=\"rnn\" style=\"height: 200px;\"/>\n",
    "\n",
    "We can generalize this to a network with many hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### UNROLLED RECURRENT NEURAL NETWORK\n",
    ")\n",
    "A RNN unrolled to few number of timesteps is a directed acyclic graph (DAG). So, we can still use backpropagation to update parameters of the network. Such, a backpropagation is called backpropagation through time (BPTT)\n",
    "\n",
    "<img src=\"images/unrolled_rnn.png\" alt=\"rnn\" style=\"height: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN recurrence in matrix form\n",
    "\n",
    "$ h_{t}^{l} = \\tanh W^{l} \\left[h_{t}^{l-1} \\; h_{t-1}^{l}\\right]^{T}  $\n",
    "\n",
    "where\n",
    "\n",
    "$ h \\in R^{n} $\n",
    "\n",
    "$W^{l}$ is $n \\times 2n $ matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
